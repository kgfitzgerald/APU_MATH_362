---
title: "Problem Set 08"
author: "SOLUTIONS"
output: 
  html_document:
    toc: true
    toc_float: true
    number_section: false
    highlight: tango
    theme: "cosmo"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(message = FALSE, warning = FALSE)
```

You can access the .Rmd for the solutions [here](https://github.com/kgfitzgerald/APU_MATH_362/blob/main/PROBLEM_SETS/SOLUTIONS/PS-08-SOLUTIONS.Rmd).

## Summary / Reflection Prompts

*RP1: What were the main concepts covered in this assignment?*


*RP2: What's one thing you understand better after completing these problems?*


*RP3: What problems gave you the most trouble? What was difficult about them/where did you get stuck?*

# By hand or typed

## Exercise 1

Note $\hat{Y}_i = a + b(x_i - \overline{x}$. 

We want to show that $\sum_{i = 1}^n(Y_i - \hat{Y}_i) = 0$

$$
\begin{aligned}
\sum_{i = 1}^n(Y_i - \hat{Y}_i) &= \sum_{i = 1}^n(Y_i - (a + b(x_i - \overline{x})) \\
&= \sum_{i = 1}^n Y_i - an - b\sum_{i = 1}^nx_i+bn\overline{x} \\
&= \sum_{i = 1}^n Y_i - an
\end{aligned}
$$

with the last equality being true because $bn\overline{x} = bn\frac{\sum_{i = 1}^nx_i}{n} = b\sum_{i = 1}^nx_i$.

Recall that $a = \overline{y}$, so 

$$
\begin{aligned}
\sum_{i = 1}^n Y_i - an &= \sum_{i = 1}^n Y_i - \overline{y}n \\
&= \sum_{i = 1}^n Y_i - \sum_{i = 1}^n Y_i \\
&= 0
\end{aligned}
$$

as was to be proved.



## Exercise 2

### Part a

Let $Y_i = \beta X_i + \epsilon_i, \ \ \ \epsilon_i \sim N(0, \sigma^2)$.

This implies $Y_i \sim N(\beta X_i, \sigma^2)$. Therefore, there are two unknown parameters ($\beta, \sigma^2$), and the likelihood function is given as follows

$$
\begin{aligned}
L(Y_i | \beta, \sigma^2) &= \prod_{i = 1}^n \frac{1}{\sqrt{2\pi\sigma^2}}exp\left[\frac{-(Y_i - \beta X_i)^2}{2\sigma^2}\right] \\
&= (2\pi)^{-n/2}(\sigma^2)^{-n/2}exp\left[\frac{-1}{2\sigma^2}\sum_{i = 1}^n(Y_i - \beta X_i)^2\right]\\
logL(\beta, \sigma^2) &= -\frac{n}{2}log(2\pi) - \frac{n}{2}log(\sigma^2) - \frac{1}{2\sigma^2}\sum_{i = 1}^n(Y_i - \beta X_i)^2
\end{aligned}
$$

To find the MLE of $\beta$, we can take the derivative of the log-likelihood function with respect to $\beta$ and set the expression equal to 0. 


$$
\begin{aligned}
\frac{\partial logL(\beta, \sigma^2)}{\partial \beta} &= 2 \sum_{i = 1}^n(Y_i - \beta X_i)(X_i) = 0 \\
&=-\sum_{i = 1}^nY_iX_i + \beta\sum_{i = 1}^nX_i^2 = 0 \\
\end{aligned}
$$

Rearranging and solving for $\beta$ yields $\hat{\beta} = \frac{\sum_{i = 1}^n Y_iX_i}{\sum_{i = 1}^n X_i^2}$

To find the MLE of $\sigma^2$, we can take the derivative of the log-likelihood function with respect to $\sigma^2$ and set the expression equal to 0. 


$$
\begin{aligned}
\frac{\partial logL(\beta, \sigma^2)}{\partial \sigma^2} &= \frac{-n}{2}\frac{1}{\sigma^2} + \frac{1}{2\sigma^4}\sum_{i = 1}^n(Y_i - \beta X_i)^2 = 0 \\
&= -n\sigma^2 + \sum_{i = 1}^n(Y_i - \beta X_i)^2 = 0 \\
\end{aligned}
$$

Solving for $\sigma^2$ and plugging in the MLE for $\beta$ yields $\hat{\sigma}^2 = \frac{\sum_{i = 1}^n (Y_i - \hat{\beta}X_i)^2}{n}$

### Part b

Note that $\hat{\beta} = \frac{\sum_{i = 1}^n Y_iX_i}{\sum_{i = 1}^n X_i^2}$ is a linear combination of normal random variables since $Y_i \sim N(\beta X_i, \sigma^2)$. Therefore $\hat{\beta}$ is also normal; we just need to find its expected value and variance. 

$$
\begin{aligned}
E(\hat{\beta}) &= E\left(\frac{\sum_{i = 1}^n Y_iX_i}{\sum_{i = 1}^n X_i^2}\right) \\
&= \frac{\sum_{i = 1}^n X_iE(Y_i)}{\sum_{i = 1}^n X_i^2}\\
&= \frac{\sum_{i = 1}^n X_i\beta X_i}{\sum_{i = 1}^n X_i^2}\\
&= \frac{\beta \sum_{i = 1}^n X^2_i}{\sum_{i = 1}^n X_i^2}\\
&= \beta
\end{aligned}
$$
$$
\begin{aligned}
V(\hat{\beta}) &= V\left(\frac{\sum_{i = 1}^n Y_iX_i}{\sum_{i = 1}^n X_i^2}\right) \\
&= \frac{\sum_{i = 1}^n X^2_iV(Y_i)}{\left(\sum_{i = 1}^n X_i^2\right)^2}\\
&= \frac{\sum_{i = 1}^n X^2_i\sigma^2}{\left(\sum_{i = 1}^n X_i^2\right)^2}\\
&= \frac{\sigma^2}{\sum_{i = 1}^n X_i^2}
\end{aligned}
$$

Therefore, $\hat{\beta} \sim N\left(\beta, \frac{\sigma^2}{\sum_{i = 1}^n X_i^2}\right)$

### Part c

With the two coordinates (1,1) and (2,1), we have 

$$
\begin{aligned}
\hat{\beta} &= \frac{\sum_{i = 1}^n Y_iX_i}{\sum_{i = 1}^n X_i^2} \\
&= \frac{(1)(1) + (2)(1)}{1^2 + 2^2} = \frac{3}{5}
\end{aligned}$$

$$
\begin{aligned}
\hat{\sigma}^2 &= \frac{\sum_{i = 1}^n (Y_i - \hat{\beta}X_i)^2}{n} \\
&= \frac{(1 - \frac{3}{5}(1))^2 + (1 - \frac{3}{5}(2))^2}{2} \\
&= \frac{\frac{4}{25} + \frac{1}{25}}{2}= \frac{1}{10}
\end{aligned}$$

### Part d

$\hat{y}_i = \hat{\beta}X_i$ and $e_i = y_i - \hat{y}_i$ so

$$\hat{y}_1 = \frac{3}{5}(1) = \frac{3}{5}$$
$$\hat{y}_2 = \frac{3}{5}(2) = \frac{6}{5}$$
$$e_1 = 1 - \frac{3}{5} = \frac{2}{5}$$
$$e_1 = 1 - \frac{6}{5} = -\frac{1}{5}$$

Therefore, $\sum_{i = 1}^n e_i = \frac{1}{5} \neq 0$

## Exercise 3

Note $a = \overline{y} = \frac{\sum_{i = 1}^nY_i}{n} \sim N(\alpha, \frac{\sigma^2}{n})$. Therefore, $$\frac{a - \alpha}{\sigma/\sqrt{n}} \sim N(0,1)$$
We also know that $$\frac{n \hat{\sigma}^2}{\sigma^2} \sim \chi^2_{n-2}$$
We know that a random variable of the form $T = \frac{Z}{\sqrt{U/df}}$ follows a $t_{df}$ distribution, where $Z \sim N(0,1)$ and $U \sim \chi^2_{df}$. Therefore we can develop a test statistic in the following way:

$$
\begin{aligned}
T &= \frac{Z}{\sqrt{U/df}} = \frac{\frac{a - \alpha}{\sigma/\sqrt{n}}}{\sqrt{\frac{n \hat{\sigma}^2}{\sigma^2}/(n-2)}} \\
&= \frac{\frac{a - \alpha}{\sigma}\sqrt{n}}{\frac{\sqrt{n}\hat{\sigma}}{\sigma\sqrt{n-2}}}
= \frac{a - \alpha}{\sqrt{\hat{\sigma}^2/(n-2)}} \sim t_{n-2}
\end{aligned}
$$

Therefore we can write the probability statement

$$P\left(-t_{\alpha/2; n-2} \leq \frac{a - \alpha}{\sqrt{\hat{\sigma}^2/(n-2)}} \leq t_{\alpha/2; n-2}\right) = 1 - \alpha$$

Rearranging to solve for $\alpha$ gives

$$P\left(a -t_{\alpha/2; n-2}\frac{\hat{\sigma}}{\sqrt{n-2}} \leq \alpha \leq a + t_{\alpha/2; n-2}\frac{\hat{\sigma}}{\sqrt{n-2}}\right) = 1 - \alpha$$

Therefore, a $(1-\alpha)*100\%$ confidence interval for $\alpha$ is given by
$$a \pm t_{\alpha/2; n-2}\frac{\hat{\sigma}}{\sqrt{n-2}}$$

*Note the unfortunate notation from the book that the intercept parameter $\alpha$ is different from the usual significance level $\alpha$.*



# Typed

## Exercise 4

```{r}
library(tidyverse)
library(tidymodels)

#read data
senic <- read_csv("https://raw.githubusercontent.com/kgfitzgerald/APU_MATH_362/main/PROBLEM_SETS/data/senic.csv")
```

### Part a

```{r}
#fit model
m1 <- linear_reg() %>% 
  set_engine("lm") %>% 
  fit(length ~ infection_risk, data = senic)
tidy(m1)

alpha_hat <- tidy(m1)[1,2]
beta_hat <- tidy(m1)[2,2]
```

### Part b

The regression equation is given by $$\hat{y}_i = `r round(alpha_hat, 2)` + `r round(beta_hat, 2)`x_i,$$ 
where $y_i$ is the length of stay in the hospital, and $x_i$ is the infection risk (average estimated probability of acquiring an infection).


### Part c

Intercept: For a hospital with an infection risk of 0, the average length of stay at the hospital is 6.34 days.

Slope: For each percentage point increase in infection risk (probability of acquiring an infection), a hospital's average length of stay increases by 0.76 days on average.

### Part d

$$H_0: \beta = 0$$
$$H_A: \beta \neq 0$$

### Part e

The test statistic is `r round(tidy(m1)[2,4], 2)`, and the p-value is `r tidy(m1)[2,5]`. Therefore, there is sufficient evidence to reject $H_0$ and claim that infection risk is associated with length of stay. 

### Part f

The test statistic is computed by $t = \frac{\hat{\beta} - \beta_0}{\sqrt{\frac{MSE}{\sum_{i=1}^n(x_i - \bar{x})^2}}}$. Note that $\beta_0 = 0$ (the value hypothesized in $H_0$ and $\hat{\beta} = `r round(beta_hat, 2)`$ from the above regression table. We can extract all the remaining relevant numbers from the following R code.

```{r}
# raw output
anova(m1$fit)

# tidy output for extracting numbers with code
tidy(anova(m1$fit))

MSE <- tidy(anova(m1$fit))[2,4]

# note the sum of squares (SS) in the denom in square root 
# of test statistic is simply the same numerator that 
# appears on the sample variance of x
# so can use the following code to compute it
n <- nrow(senic)
SS <- var(senic$infection_risk)*(n - 1)
SS

# alternatively, to compute sum of squares (SS) "manually": 
SS_alt <- senic %>% 
  mutate(diff_sqrd = (infection_risk - mean(infection_risk))^2) %>% 
  summarize(SS = sum(diff_sqrd)) %>% 
  pull(SS)
SS_alt

T <- (beta_hat - 0)/sqrt(MSE/SS)
T
```

Therefore, 

$$\hat{\beta} = `r round(beta_hat, 2)`$$

$$\beta_0  = 0$$

$$MSE = `r round(MSE, 2)`$$
$$\sum_{i=1}^n(x_i - \bar{x})^2 = `r round(SS, 2)`,$$

so we have $$T = \frac{`r round(beta_hat, 2)` - 0}{\sqrt{\frac{`r round(MSE,2)`}{`r round(SS,2)`}}} = `r round(T, 2)`$$

