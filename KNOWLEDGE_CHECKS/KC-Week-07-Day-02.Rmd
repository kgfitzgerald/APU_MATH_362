---
title: "Week 07 Day 02"
subtitle: ""
output: 
  html_document:
    toc: true
    toc_float: true
    number_section: false
    highlight: tango
    theme: "cosmo"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(message = FALSE, warning = FALSE)
```

## Student Questions

+ $\hat{\mu} = \frac{\sum_{i = 1}^n X_i}{n} = \overline{X}$. What's the difference between $X_i$ and $\overline{X}$?

+ $E(u(X_1, X_2, ..., X_n)) = \theta$. So we just want to know if the expected value of the mean of random samples is equal to the parameter space? So we are just checking to see if the expected value is part of the space and if it is not then it is biased?

+ $E(X_i) = p$ for a Bernoulli random variable. How do we know this again? 

+ "$E(\hat{p}) = p$, therefore the maximum likelihood estimator is an unbiased estimator of $p$."
    + Student question: And this holds true no matter what right? It doesn't depend on any specific values. That's just a truth about parameter p?
    
+ Got lost on step in finding $E(\hat{\sigma}^2)$

+ Since this is estimator is biased, do we search for an alternative estimator for the variance? Is there ever a situation where we have to settle for the biased estimator?

+ Why is $S$ not an unbiased estimator of $\sigma$ if $S^2$ is an unbiased estimator of $\sigma^2$?? 
    + Proof: [https://stats.stackexchange.com/questions/11707/why-is-sample-standard-deviation-a-biased-estimator-of-sigma](https://stats.stackexchange.com/questions/11707/why-is-sample-standard-deviation-a-biased-estimator-of-sigma)
    
+ What's the difference? Did we see these in 361?
  + $E(X^k)$
  + $E[(X-\mu)^k]$
  + $M_k = \frac{1}{n}\sum_{i =1}^nX_i^k$
  + $M_k^* = \frac{1}{n}\sum_{i =1}^n(X_i-\overline{X})^k$

+ Will the method of moments always give the same estimator as the maximum likelihood estimator? Or is this just a coincidence?

+ How do we know in which cases to find moments about the origin and in which cases to find the sample moments about the mean? Is it some arbitrary decision that we just have to make? Are these forms of the method equal (i.e. will they give the same answers), the only difference being that one makes the math a little easier to work with?
    + Book: "This example, in conjunction with the second example, illustrates how the two different forms of the method can require varying amounts of work depending on the situation." 
    + Student: "I'm still a little lost on why you use method of moments vs the other method"

+ In the alternative version of Method of Moments, why is the first step to set the first sample moment equal to the moment about the origin, instead of the moment about the mean? 
  + [https://online.stat.psu.edu/stat415/lesson/1/1.4#paragraph--500](https://online.stat.psu.edu/stat415/lesson/1/1.4#paragraph--500)
  + So is the only difference that this method takes the second sample moment about the mean instead of about the origin? When would we use the difference methods and why?
  
+ How do we know $E(X_i) = \alpha\theta$? [in that particular context] 
    + Wikipedia for gamma: [https://en.wikipedia.org/wiki/Gamma_distribution](https://en.wikipedia.org/wiki/Gamma_distribution)

+ $\hat{\theta}_{MM}$ - are the MM used to denote the method of moments estimator? 

## Knowledge Check

+ Mathematically, what does it mean for an estimator to be unbiased?
+ Recall properties of expected value. How can you re-write/reduce the following, where $a$ and $b$ are constants?
    + $E(aX)$
    + $E(aX + b)$
    + $E(\sum_{i = 1}^naX_i)$
    + $E[\sum_{i = 1}^n(aX_i + b)]$
    


    